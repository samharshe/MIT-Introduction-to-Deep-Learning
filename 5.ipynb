{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lecture 5: Robust and Trustworthy Deep Learning](https://www.youtube.com/watch?v=kIiO4VSrivU&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Themis AI.\n",
    "work comes straight from the bleeding edge of ML research. yeah yeah so does mine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problems with AI are a combination of problems with bias and uncertainty.  \n",
    "what happens when models are skewed by sensitive feature inputs?  \n",
    "can we teach a model to recognize when it doesn't know an answer?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bias in the AI lifecycle:  \n",
    "what the AI is trained on is not what it is deployed on in some way.  \n",
    "deployment bias: the population changes in some way after the model is deployed.  \n",
    "evaluation bias: the model is ranked by its ability to solve problems that are not representative of the problems it will actually be solving.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some facial detection classifier models do barely better than random on populations it was not trained on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what if not all classes appear equally frequently in your dataset?  \n",
    "ex more negatives than positives.  \n",
    "mitigate by:  \n",
    "1. sample reweighting. sample more data points from underrepresented classes.  \n",
    "2. loss reweighting. mistakes on underrepresented classes count more toward loss function.  \n",
    "3. batch selection. choose randomly from classes, not from the dataset as a whole.\n",
    "\n",
    "what does 1 mean if not 3? why would we want *more* from underrepresented classes? shouldn't we want the same from each class?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "debiasing latent features is difficult.  \n",
    "1. we don't know which features are biased, so we don't know which features to label.  \n",
    "2. it is expensive to label so many features.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use VAE to learn latent structure and mitigate bias.  \n",
    "1. learn latent structure.  \n",
    "2. estimate distribution of latent space.\n",
    "3. oversample from sparser areas of distribution and undersample from denser areas.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "types of uncertainty in neural networks:  \n",
    "1. data uncertainty. data in some area has no clear distribution.  \n",
    "2. model uncertainty. there is no data in some areaâ€”points are \"out of the distribution.\"  \n",
    "model uncertainty can be reduced by adding data to areas with high model uncertainty.  \n",
    "data uncertainty is irreducible.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data uncertainty is \"aleatoric\" uncertainty. irreducible!  \n",
    "model uncertainty is \"epistemic\" uncertainty. we can reduce it by learning more! cannot be learned directly from the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimating aleatoric uncertainty: output not only a prediction, but a variance of the neighborhood of the input.  \n",
    "we change loss function to negative log likelihood (NLL), which accounts for the variance, so our model learns the variance in different neighborhoods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimate epistemic uncertainty by asking identically architected NNs with different initial values to make predictions. if they all give the same prediction, we have low model uncertainty. we are familiar with inputs of this kind. if they give different predictions, we have high model uncertainty. we are not familiar with inputs of this kind.  \n",
    "called \"ensembling\": we make predictions with an ensemble of models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is compute-intensive, though.  \n",
    "the key insight is that we can estimate epistemic uncertainty by introducing stochasticity.  \n",
    "a cheaper way to introduce this stochasticity is to use dropout layers.  \n",
    "normally, we only use dropout layers during training. we keep all layers at testing so as not to lose any information.  \n",
    "here, though, we use dropout layers in testing.  \n",
    "run multiple forward passes on each test instance.  \n",
    "mean prediction is prediction.  \n",
    "variance between predictions tells us our epistemic uncertainty."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also use VAEs to estimate epistemic uncertainty. if the decoder doesn't understand the latent vector, then we have something weird, something not similar to what we've seen before. this tells us we have high epistemic uncertainty in this area."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAPSA is a single line that makes the model not only output a prediction but its biases, uncertainty, and label noise.  \n",
    "wraps the model to make the minimum necessary modifications and preserve the capabilities.   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reflections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aleatoric uncertainty is irreducible. it comes from the data being high-variance in some area.  \n",
    "epistemic uncertainty is reducible. it comes from a lack of data in some area.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we deal with bias in datasets by finding the distribution over the latent variable space and choosing training examples uniformly throughout the space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sometimes, it is very useful to knownot only the prediction, but the variance in the neighborhood, and the bias of the model as a whole. CAPSA gives us this without being too computationally intensive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout is really cool and it lets us estimate epistemic uncertainty and also (as we previously learned) to guard against overfitting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
