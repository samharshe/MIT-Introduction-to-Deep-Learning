{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tutorial 31- Back Propagation In Recurrent Neural Network](https://www.youtube.com/watch?v=6EXP2-d_xQA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the math here looks like it suckssssss. I want to have a better intuition for the interdependencies here, but I'll only memorize the exact sequence of the matrix operations if I need to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is weird because we're iterating—back-propagating—but there's only one set of weights that we're modifying."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's just simple chain rule stuff. annoying but not complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Backpropagation in RNN | Backpropagation through time](https://www.youtube.com/watch?v=Wu9BsMSKqyk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interlude: I review log loss. why is this so badly explained everywhere? just take the log of the difference between your predicted probability of the right answer and 1. the other values do not matter at all, since they are being multiplied by 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't understand this W_aa and W_ya stuff because i thought we had one weight matrix we were using the whole time. what i need, then, is to understand RNN architecture itself better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Recurrent Neural Networks - Ep. 9 (Deep Learning SIMPLIFIED)](https://www.youtube.com/watch?v=_aCuOwF1ZjU)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanishing gradient problem worse for RNNs because each timestep is equivalent to a layer in a FFNN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rewatching [MIT 6.S191: Recurrent Neural Networks and Transformers](https://www.youtube.com/watch?v=QvkQ1B3FBqA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE DO NOT ONLY NEED TO ACCOUNT FOR THE RELATIONSHIP BETWEEN THE LOSS AND THE WEIGHTS. WE NEED TO ACCOUNT FOR THE RELATIONSHIP BETWEEN THE INTERNAL STATE AND THE WEIGHTS. THIS IS THE EXTRA THING I WAS MISSING. EACH INTERNAL STATE IS AN INPUT INTO THE MODEL AT THE NEXT TIMESTEP, SO WE NEED TO CALCULATE HOW INCORRECT (\"INCORRECT\") EACH INTERNAL STATE WAS JUST AS WE CALCULATE HOW INCORRECT (\"INCORRECT\") EACH NEURON IS IN A FFNN. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it feels good to keep studying even though I sorta get it because I swear I'm missing something, then to discover, yes, I was missing something! I did not delude myself!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "although maybe I still don't understand it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can say with confidence that I think I understand it, though, so I am content to move on to the next lecture."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
