{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lecture 7: Deep Learning Limitations and New Frontiers](https://www.youtube.com/watch?v=FHeCmnNe0P8&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universal Approximation Theorem:  \n",
    "a sufficiently large single-layer FFNN can approximate to arbitrary precision any continuous function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caveats:  \n",
    "the number of units in that layer may be infeasibly large.  \n",
    "the resulting model may not generalize."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with images and RANDOM labels shows that NNs can perfectly fit to completely random data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNs here are perhaps TOO GOOD as functional approximators. they can fit to a random function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning is NOT alchemy. GIGO applies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNs can act weird under perturbations.  \n",
    "we know the gradient, so we can modify an image so as to maximize the increase in the loss of the NN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ongoing work to introduce structure and prior human knowledge into NN architectures.  \n",
    "also ongoing work to help NNs extrapolate better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one example we already know of: CNNs.  \n",
    "we use multiple filters to extract local data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Convolutional Networks (GCNs):  \n",
    "kernel slides around on graph and extracts features from the neighborhood of each node.  \n",
    "kernel extracts information about local connectivity from each node.  \n",
    "applications to traffic patterns, molecular discovery, COVID-19 forecasting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can extend CNNs to 3d by using the same idea on Pointclouds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "limitations of VAEs and GANs:  \n",
    "1. mode collapse. they keep generating examples that are very similar to each other. rarely generate data that is not very similar to training data.  \n",
    "2. hard to train. unstable. inefficient.  \n",
    "goals:  \n",
    "1. stability.  \n",
    "2. efficiency.  \n",
    "3. quality.  \n",
    "4. novelty.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion Models generate new samples iteratively by refining output and removing noise little by little.  \n",
    "two key parts.  \n",
    "1. forward noising.   \n",
    "take some image.  \n",
    "add noise to it little by little until it's pure noise.  \n",
    "2. reverse denoising.  \n",
    "then we learn a mapping to go from pure noise to the original image.  \n",
    "learn to recover information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noising function is NOT a neural network.  \n",
    "reverse denoising is ML. given an image at $T_n$, can we estimage the image at $T_{n-1}$?  \n",
    "loss is given my pixelwise mean squared difference.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the power is that the stochasticity in the input allows us to generate very diverse samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you define the number of timesteps up front."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same idea can be applied to things other than image generation. the whole idea is recovering information from randomness. generating molecules is one example. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use diffusion model to predict folded protein structure from unfolded structure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reflections:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diffusion models are wicked cool. they learn to undo noising to generate examples from randomness. they have a wide range of applications and generate more diverse examples than VAEs and GANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCNs apply CNN architecture to graphs and have appliations wherever graphs are used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNs act weird under perturbations, especially when these perturbations are designed specifically to maximize the increase in the loss function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNs can learn arbitrary continuous functions with arbitrary precision, but not necessarily quickly. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
