{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lecture 1: Introduction to Deep Learning](https://www.youtube.com/watch?v=7sB052Pz0sQ&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from a rhetorical perspective, the Obama bit is phenomenal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't understand very well why  creating fake environments to train models in works. aren't we limited by the same data?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deep learning ~ neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do we ensure the neural network's layers correstpond to sensible abstractions from the data? do we care?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why do NNs work now?  \n",
    "1. more good data. (why are NNs so data-intensive?)  \n",
    "2. hardware is improving (GPUs and parallelization).  \n",
    "3. software is super easy to use. (this seems like a consequence of 1 and 2)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum of inputs times weights plus bias put through a non-linear (logistic, right?) function and finally output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\hat{y} = g(w_0 + \\Sigma^m_{i=1} x_iw_i) $, where g is our non-linear function, w0 is our weight, xiwi is an input times a bias, and Å· is our prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in linear algebra: $ \\hat{y} = g(w_0 + X^TW)$, where X and W are our inputs and weights conveniently organized in vectors. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why do we use the sigmoid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.7310586, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant([1.0])\n",
    "print(tf.math.sigmoid(x)[0])\n",
    "\n",
    "# it works!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the point of activation functions is to introduce nonlinearity into the model. with a linear activation function, you will only be able to draw lines.  \n",
    "with nonlinearities, we can approximate arbitrarily complex functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is in g is always just a line, taking the inputs X as variables. the bias affects where the line hits the axis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dot product + bias + nonlinearity = perceptron.  \n",
    "can we call it a neuron? or a node? I can't take \"perceptron\" seriously."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to add outputs, just add another perceptron and calculate its output analagously."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense layers are ones in which every input is connected to every output.\n",
    "deep neural network: just keep stacking layers on top of each other."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function: $L(f(x^{(i)}, W), y^{(i)})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first input is the output of the model.  \n",
    "the second input is the actual value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "empirical loss is the loss over the entire dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different loss functions for different types (eg binary classification vs regression) of models. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to change the weights so as to minimize the output of the loss function over the entire dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with only a few weights, we can plot J(W) (the empirical loss function) and simply search for the global minimum.  \n",
    "with more weights, it is too expensive to do this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use gradient descent to walk toward values for our weights such that ourcost function is smaller."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have to choose step size / learning rate to make sure we get down the hill quickliy but don't overshoot the bottom."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the chain rule to find an equation giving a product of partial derivatives of every layer from the input to the output that gives the gradient of the function with respect to the weights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want learning rates large enough to skip over local minima."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"adaptive\" learning rates change based on their environment. (?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient is computed over your entire dataset and is therefore very expensive.  \n",
    "stochastic gradient descent updates over a single example and is therefore cheap but noisy.  \n",
    "small-batch computes over a...small batch of examples and is not too noisy but not too expensive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more accurate estimation of gradient makes us update our model more accurately."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularization discourages complexity in the model.  \n",
    "most common regularization technique is dropout.  \n",
    "force some number (say, 50%) of nodes in each layer to be inactive in any given round so that the model doesn't rely on one node too much.  \n",
    "next most common regularization technique is early stop.  \n",
    "see where the training loss and test loss start divergin and stop fine-tuning the model.  \n",
    "any modification to the model beyond this is overfitting.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reflections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this was all good review. I need to understand better: the purpose of the sigmoid function in introducing nonlinearity;different regularization techniques; and adaptive learning rates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
