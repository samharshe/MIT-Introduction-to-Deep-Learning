{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lecture 4: Deep Generative Modeling](https://www.youtube.com/watch?v=QcLlc9lj2hk&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9N)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deep learning can not only detect patterns, but generate new instances based on those patterns.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "supervised learning is functional: find a map from input space to output space.   \n",
    "unsupervised learning is not: we want to figure out the hidden, underlying structure of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn a model that represents the distribution (presumably in some crazy high-dimensional space) of the training samples.  \n",
    "1. density estimation.  \n",
    "2. sample generation.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "latent variable models:  \n",
    "1. autoencoders and variational autoencoders (VAEs).  \n",
    "2. generative adversarial networks (GANs).  \n",
    "\n",
    "but what is a latent variable?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autoencoders: build some encoding of the input and try to reconstruct the input directly.  \n",
    "map from the input x to some lower-dimensional latent space z.  \n",
    "(this is just learning some compression algorithm.)  \n",
    "decoder network takes z and tries to reconstruct $\\hat{x}$ with it.  \n",
    "train this network by teaching it to minimize $||\\hat{x} - x ||$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the lower the dimension of the latent space, the worse the reconstruction quality.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAEs are more commonly used than traditional auto-encoders today."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAEs introduce stochastic element to allow model to generate values not seen in the training data.  \n",
    "this also allows the transition to the latent space z to be smoother. (how?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with VAEs, we break down the latent space into a mean vector and a standard deviation vector.  \n",
    "the encoder network therefore wants to output a distribution of the variables in the latent space.  \n",
    "decoder network wants to output a distribution of the variables over the input space given the distribution over the latnet space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we add a regularization term D to the loss function to keep the inferred latent distribution from differing too dramatically from our prior latent distribution.  \n",
    "in other words, we have an initial guess for what the latent space should look like, and we penalize the model for looking too different from that.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common choice for the prior: a standard Gaussian distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL-divergence is the metric we use for computing the penalty given by the regularization term D.  \n",
    "it is a way of calculating how different two distributions are."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what properties do we want from regularization?  \n",
    "1. continuity: close in latent space means close in semantic or functional space.  \n",
    "2. completeness: sampling from latent space means something after reconstruction.  \n",
    "normal prior helps us achieve these properties, which is why we choose it.  \n",
    "\n",
    "the addition of this stochastic term means that we need some clever innovation to recover the ability to do back-propagation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we consider $\\mu$ and $\\sigma$ fixed and attribute all the stochasticity to a term $\\epsilon$ from the normal distribution.  \n",
    "we \"divert\" the stochasticity away from the $\\mu$ and $\\sigma$ that we want to learn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can therefore change the value of one latent variable in particular."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want latent variables to pick up on completely different things. \"disentanglement.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we achieve this by making the coefficient of the regularization term of our latent encoding sufficiently high."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs!  \n",
    "one network tries to find some transformation from random noise to the output space.  \n",
    "another network tries to tell whether the outputs are artificial or whether they are genuine samples from the data.  \n",
    "the generator learns to generate outputs that are more and more likely to be classified as real.  \n",
    "the discriminator becomes better andbetter attelling what is real and what is fake.   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traversing the random input space corresponds to traversing the generative output space, which is really cool!  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea: add layers and layers to both the discriminator and generator sothey both become more sophisticated.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can transfer the style of a target image onto a new output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conditioning allows us to add a label to tell the network what kind of output we want (e.g. segmentation of some scene, aerial to map image, etc.)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with CycleGANs, we can go from any arbitrary manifold to another. instead of random input space to coherent output space, we can go from \"cartoon image space\" to \"photorealistic image space\" or something like that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reflection:\n",
    "to generate new instances of data, we map input data to a latent space that we hope captures all the essential axes along which our data can vary, then choose from the latent space distribution, then undo the latent space mapping to recover the original input (pixels or whatever).  \n",
    "\n",
    "GANs have one network that is trying to get better and better at producing fakes and another that is trying to get better and better at spotting them.  \n",
    "\n",
    "CycleGANs allow us to go from any arbitrary manifold to another. so, for example, we can synthesize speech by doing a transformation on the spectrometer reading of that speech, which is really a kind of image modification."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
