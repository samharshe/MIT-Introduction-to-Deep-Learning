{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lecture 3: Convolutional Neural Networks](https://www.youtube.com/watch?v=uapdILWYTzE&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how can we give computers a sense of sight?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the goal of vision is to predict."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "watching Boston Dynamics robots dancing was an excellent way to spend a couple of minutes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to determine what's in an image, identify key features in the image category, like eyes/nose/mouth, door/windows/steps, and so on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do we ensure the model is using the right abstractions? how do we know what the right abstractions are?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we would like the model to learn what a human face comprises just by looking at lots of human faces."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we lose all spacial structure when we flatten our image and put it into a FFNN as a row vector of pixel values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep out image in 2-d and only allow each node in our hidden layer to see a small part of our image.  \n",
    "slide this small patch across the whole image. this is called convolution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each feature is a mini-image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolution takes as an input two matrices. in our context, this means taking in two images and outputting an image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiply element-wise to see the relationship between the corresponding cells in the scanner and in the image.  \n",
    "this creates a feature map, each of whose cells is larget the more similar the filter is to the sub-image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn a bunch of features by training on a bunch of different filters.  \n",
    "how do we determine what the filters are? don't we run into the same problem of humans being bad at abstraction?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs for classification:  \n",
    "1. input image.  \n",
    "2. convolution (feature maps). this is what we just went over.  \n",
    "3. maxpooling. (this should be covered next, I think.) downscale our features so our filters attend to a larger part of our original image. I think this means running filters over filtered images again.  \n",
    "4. fully-connected layer.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same patch can be passed through multiple filters, creating a stack of images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU activation function is often the way we introduce nonlinearity.\n",
    "ReLU = $max(0,x)$. don't let these nerds pretend it's complicated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 above on maxpooling is wrong. maxpooling is literally choosing the max value in a cell from each pool so that we can condense the image further."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. learn features in input images through convolution.  \n",
    "2. introduce non-linearity through activation function (often ReLU).  \n",
    "3. reduce dimensionality and preserve spatial invariance with pooling.  \n",
    "and repeat and repeat until you have your output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we downscale our receptive field, we can increase the number of features we are trying to learn.  \n",
    "computational trade-off: we don't want to run a bunch of filters over a very high-def image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs can be used for object detection, segmentation, regression, etc.  \n",
    "the first part of the model is just learning what's in the image.  \n",
    "once we have predictions for what's there, we can use that information however we want."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to be flexible to detect a variable number of objects of variable size."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-CNN algorithm: first find regions that we think have objects. then use a CNN to classify them. problem: slow and brittle because the detection and classification are disconnected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "faster R-CNN model:  \n",
    "feed the image into a region proposal network that draws boxes where there might be interesting stuff for us to classify.  \n",
    "independently process each region with its own feature extraction head.  \n",
    "how does this solve the problem? aren't we still separating the RPN from the classification model?  \n",
    "a: somehow the RPN shares the convolutional features with the CNN. I don't really understand, but I'm not meant to. the idea is that the detection and the classification should be simultaneous, and that is noted."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reflections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs allow us to abstract certain features from images.  \n",
    "they preserve the 2-d structure of the input.\n",
    "I do not understand hwo we know what filters to use: does the programmer design them himself? this seems unlikely. does he just decide how many to use and let the image decide what they look like?  \n",
    "we pass our filter/scanner over the entire image and create a new image with cells corresponding to the element-wise relationship between the filter and the scanned part of the image.  \n",
    "we maxpool to shrink.  \n",
    "we do the same thing again, adding to our number of filters at each step if we so choose. we should do this because the number of higher-level abstractions is greater than (for example) the number of different ways an edge can be oriented. and also sincewe can do his because this is not too computationally expensive because the patch that we are scanning shrinks because of maxpool.  \n",
    "once we have extracted the right features, we can do whatever we want: regression, classification, segmentation, or anything else. the backend of this model does not depend on the front.\n",
    "\n",
    "I need to understand better how we choose/learn the filters.\n",
    "this will be hard to implement myself, but the exercise will be excellent, so I will do it soon. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
