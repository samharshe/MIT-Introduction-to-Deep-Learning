{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lecture 6: Deep Reinforcement Learning](https://www.youtube.com/watch?v=AhyznRSDjw8&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=6)\n",
    "(I realized that I switched to the 2023 lectures part-way through the series. I'll keep on with the 2023 lectures because the topics are the same and I'd rather have the newer version.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea of lecture: marry deep learning with reinforcement learning.  \n",
    "learning in dynamic environments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we no longer have only a fixed dataset. fixed datasets are unlike the real world. we want our model exploring and interacting with the world and intaking data all the while."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL is a big deal in robotics and in gameplay."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL: data given only in the form of state-action pairs.  \n",
    "states are observations.  \n",
    "actions are the behaviors that the agent takes.  \n",
    "goal: maximize future rewards over many time steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is the agent and there is the environment.  \n",
    "agent takes actions in the environment.  \n",
    "*A* is the action space: the set of all possible actions that the agent could take.\n",
    "environment sends back observations to the agent. action changes state of environment. environment tells agent its new state. agent takes action again. etc.  \n",
    "reward tells the agent how good the action has been.\n",
    "we calculate the sum of all rewards infinitely into the future, discounting future rewards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$R_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+1} + \\ldots$\n",
    "total reward, $R_t$, is the discounted sum of future rewards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-function takes in the current state and a possible action and calculates the expected total future reward.  \n",
    "if we know our Q-function perfectly, we just choose the action from *A* that maximizes future reward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the meat of RL is determining the Q-function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "value learning:  \n",
    "Deep-Q Neural Network (DQN):  \n",
    "state, action go into a DNN, and out comes Q.  \n",
    "another way: state goes into a DNN, and out comes a set of Q-functions, one for each action. we choose the action *a* such that the output of all these Q-functions is maximized."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can define a target Q-value, the one that we get if we choose the best action every time.  \n",
    "we can also define a predicted Q-value given an action.  \n",
    "we want to minimize MSL between them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resummarize:  \n",
    "our NN takes in a state.  \n",
    "for $n$ different possible actions, our NN gives out $n$ outputs, each of which is a Q-function given action $a_i$.\n",
    "policy function takes in set of Q-functions and tells us what action to take so as to pick out the best Q-function.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning naturally applicable in discrete action spaces.  \n",
    "cannot handle large, continuous action spaces.  \n",
    "policy is deterministically computed given action space and current state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lack of stochasticity here is dangerous. we might keep picking the same action over and over and never explore much of the action space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy gradient: calculate the probability that an action maximizes our Q-function.  \n",
    "choose among actions based on this probability.  \n",
    "we can also use this for a continuous action space.  \n",
    "given a state, output a normal distribution. y-axis is probability that an action maximizes Q. x-axis is the parameters of our action space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case study in training policy gradients: autonomous vehicles.  \n",
    "agent: vehicle.  \n",
    "state: lidar, radar, camera, other sensors.  \n",
    "action: steering wheel angle, gas, break.  \n",
    "reward: distance traveled (?)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training algorithm:  \n",
    "1. initialize the agent.  \n",
    "2. run a policy until termination.  \n",
    "3. record all states, actions, rewards.  \n",
    "4. decrease probability of all actions that occurred near termination because they were probably not very good.  \n",
    "5. increase probability of all actions that occured far from termination.  \n",
    "(4 and 5 are noisy. we could have taken some bad actions at the beginning and some good actions near the end. oh well. this is the best we can do.)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss = $-logP(a_t|s_t)R_t$  \n",
    "part after - and before $R_t$ is log-likelihood of action.  \n",
    "$R_t$ isthe reward.  \n",
    "we plug this loss function into our gradient descent algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one problem is that we don't want our agents running until termination all the time in the real world. sometimes, we can simulate training environments, but there may be a lack of transferrability between behavior in the simulated environment and in the target environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlphaGo uses RL. first, train on some human data. then play against itself, punishing the actions that led to a loss and rewarding those that led to a victory.  \n",
    "turned out to be better without any human data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(doing more reading on RL because I don't think I understood the Q-function and policy learning stuff all that well.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN: calculate the Q-function for different actions and choose the best action.  \n",
    "policy learning: calculate the probability that different actions result in the maximal Q-function.  \n",
    "two very similar ideas, but approaching the solution from different directions. one gets a list of Q-functions given actions. the other gets a list of probabilities given actions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reflection:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use RL to get data and train on it at the same time.  \n",
    "we operate in state-action pairs: we have the environment and we act on it to modify it somehow. after each action we calculate expected reward. we want to maximize discounted future expected reward."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
